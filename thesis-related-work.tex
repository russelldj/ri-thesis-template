% -*- mode:LaTex; mode:visual-line; mode:flyspell; fill-column:75-*-

\chapter{Related Work} \label{chapRelated}



%After these quantities are estimated, it's common to re-estimate the correspondences between images using the existing solution as a way to filter incorrect correspondes which are not consistent with the majority of other matches. These steps are often the focus of extensive engineering effort to improve the final performance. 
%
%After this iterative process converges, the result is a sparse output consisting of the camera locations and calibration parameters as well as a sparse set of 3D locations. This sparse representation doesn't capture the full geometry of the scene, since only the most distinctive points are represented. Moreover, because it is a pointcloud, we cannot reason about which parts of the scene would obstuct or "occlude" the view of another one. To accomplish these tasks, we leverage the fact that many solvers can produce a mesh representation of the surface of the scene. A mesh is a common representation in computer vision and graphics, which consists of a set of 3D points connected by triangular faces. 
%
%The process of mesh computation varies by solver. A common first step is to estimate the likely depth to the surface from each camera using color constancy. This means that for a set of posible distances, the observed color from that camera is checked against the color which would be observed by the other camera, if that were the true location of the surface. From this information, a predicted most likely depth surface is predicted for each pixel. Then, the predicted depth is combined across all cameras to produce a consistent 3D geometry. This is often done using a truncated signed distance function representation \cite{} 





\section{Detecting Trees from Data at Multiple Scales}
A key first step in many ecological modeling applications is understanding the location and extent of individual trees. Because of this, the problem of detecting trees in drone and satellite data has received significant attention. There are numerous approaches for this task and they can be categorized based on whether they use geometric or visual information about the scene. 

Geometric approaches can take different data as input. A common input is a canopy height map (CHM), which is a 2D top-down representation where each location has a height. This representation is used by Popescu et al. \cite{Popescu2004SeeingHeight} and they apply a sliding window filter to identify tree locations. Other works use point clouds derived from LiDAR, which provides full 3D information about the scene. One such approach by Xiao et al \cite{Xiao2019MeanData} uses the mean shift algorithm to identify clusters of points corresponding to a given tree. This work provides a thorough assessment of the design considerations of using this algorithm.

There are also a diverse set of approaches for detecting trees in visual data. For satellite data, they often rely on ad hoc methods or generic object detection tools provided by proprietary software. 
For example, Hulet et al. \cite{Hulet2014UtilizingWoodlands} used a multi-resolution segmentation algorithm \cite{Baatz2000MultiresolutionS} to segment Pinon and Juniper trees from one-meter resolution aerial data from the National Aerial Imagery Program (NAIP) \cite{U.S.DepartmentofAgriculture2011NationalSheet}. This was followed by morphological operations that were manually tuned to be site-specific.
For drone data, there has been an increasing trend toward using deep learning. One widely-used approach is DeepForest \cite{Weinstein2020DeepForest:Delineation}, a tree detector model build on RetinaNet \cite{Lin2020FocalDetection} and trained on
a diverse set of annotations from the National Ecological Observation Network sites
\cite{Keller2008ANetwork} across the US. This model has been shown to generalize well to a variety of settings and can be further improved by fine-tuning on a small number of annotations from the local region. Importantly, this model is released in a sophisticated Python package that handles common tasks related to pre-processing, training, inference, and visualizations. One limitation is it only predicts axis-aligned bounding boxes, rather than more detailed representations such as masks. A more recent approach called DetectTree2 \cite{DetectTree2} addresses this limitation by training a Mask-RCNN \cite{He2017MaskR-CNN} model to predict tree boundaries. The limitation of this approach is it was trained on much less data, due to the scarcity of mask annotations, and has less developed software infrastructure.

The goal of our work is not to directly improve upon these techniques but rather to characterize the performance of existing methods. Specifically, we are interested in applying the same deep learning model to both drone and remote sensing data. This will allow us to quantify the difference in quality between these two sources of data. Similar themes are addressed by Fraser and Gongalton \cite{Fraser2021AImagery}, where they compare the quality of species classification and tree detection from drone and NAIP data. For tree detection, they use different learning-free algorithms for each modality. Similar to Hulet et al., they use a multi-resolution segmentation algorithm \cite{Baatz2000MultiresolutionS} for detecting trees in NAIP data. For the drone data, they use a marker-controlled water-shed segmentation technique \cite{Chen2018ReviewEvaluation}. In our work, we wish to explore deep learning, rather than classical approaches, because it generally achieves high performance and can be easily adapted to a given domain by fine-tuning. Furthermore, we wish to use the same approach on both modalities and control for as many confounding factors as possible, rather than using different methods.



\section{Mapping Forest Fire Fuel}
The goal of this work is to localize the vegetation that could become fuel for a wildfire using a drone. This is motivated by the concept for a forest management system proposed by Couceiro et al. \cite{Couceiro2019SEMFIRE:Systems}, in which a team of drones provide situational awareness to an automated ground vehicle. This vehicle is equipped with a mechanical attachment that can grind vegetation and render it much less flammable. The drones are much more maneuverable than the ground vehicle, and can thereby quickly inform it where fuel is and what regions are traversable. As a first step toward this ambitious system, the same team implemented a fuel mapping approach onboard an automated Bobcat, a utility vehicle commonly used in forestry \cite{Andrada2022IntegrationRobotics}. This approach used a LiDAR to obtain 3D information about the world and a multi-spectral camera to interpret what is fuel. Using deep learning, they identified where clumps of fuel are and then localized them in 3D using interpolated LiDAR information. A shortcoming of this approach is that it represents the fuel as a single point. In this work, we wish to represent the full 3D structure of the fuel, as well as other classes of objects in the environment, to provide more context to the ground vehicle. 

This type of problem is often termed "semantic mapping" in the robotics literature and a survey of the field is presented by Kostavalis et al. \cite{Kostavelis2015SemanticSurvey}. It consists of building a model of the world that captures both the geometry of the scene and what class each part of the scene is. To complete this task, most approaches rely on an external localization or SLAM to estimate the position and orientation of the robot.

One common framework for semantic mapping is Kimera \cite{Rosinol2020}, which relies on data from stereo cameras as input. Using estimated depth from the stereo camera, this method builds a mesh-based representation of the environment. Semantic classification information is added to the mesh and both the geometry and classification can be updated as new information arrives. We hypothesized that Kimera would be poorly suited to forest environments because it was primarily tested on built scenes with large, solid objects. In contrast, forested environments have many highly-textured surfaces and regions, such as canopies, that are not entirely solid. Therefore, we expected that a mesh-based representation would struggle to capture these extremely fine details.

An alternate class of approaches use sensors that capture explicit 3D information, such as LiDAR or RGB-D cameras. A modular approach for RGB-D semantic mapping is presented by Zhang and Fillat \cite{semantic_slam_RGBD}. In this work, they use an image-based semantic segmentation approach to identify the different classes in the scene. Then, they identify the location of each pixel in 3D space relative to the camera using the depth channel from the RGB-D sensor. Each point is assigned a class from the semantic segmentation image. This local point cloud is transformed into the global coordinate system using the estimated location and orientation of the sensor from the SLAM system. Finally, the information from each semantic point cloud is added to an octomap \cite{hornung13auro} representation, which is an efficient probabilistic volumetric representation.  This octomap is updated as new information is observed. Our previously-published work \cite{RussellUnmannedMitigation} extends this method to use a LiDAR instead of RGB-D and applies it to the fuel mapping task from drone data.

%One relavent approach is that of Zhang and Fillat \cite{XuanZhang2018Real-timeCamera}
%A significant fraction of semantic segmentation works were evaluated on datasets related to autonomous vehicles, such as CityScapes \cite{Cordts2016} or primarily urban and indoor settings such as ADE20K \cite{Zhou2017}. However, these methods have also been shown to work in natural settings. In their work, Nogueira et al. \cite{Nogueira2017SemanticConvNets} show that an ensamble of convolutional neural networks for semantic segementation can be used to effectively classify vegetation types from drone imagery.  

%However, these methods have been shown to be applicable to other domains, and recent work has explored them in the context of drone forestry images \cite{Nogueira2017SemanticConvNets, Neves2020SemanticU-net}. 

%The end goal of an automated forestry system is often to produce a map of where different classes are. Since the predictions are initially made on each image, an important step is inferring the 3D location of these predictions from the 2D image and also resolving disagreeing predictions for the same location. This challenge has been addressed by many approaches in both geospatial and robotics research. 

%Previous approaches such as Davila et. al. use a high-precision GPS-INS to register images \cite{Davila2022ADAPT:AI} to the ground plane for the task of segmenting ice in a frozen river. 
%Another approach uses predictions from multiple viewpoints registered with photogrametry likely class for the problem of wetland mapping 
%\cite{Liu2018DeepClassification}.

%In a robotics context, this problem is often called semantic mapping and there are many different approaches that assume different sensors, goals, and domains \cite{Kostavelis2015SemanticSurvey}.
%A common toolbox for generic semantic mapping Kimera \cite{Rosinol2020} that implements a set of modular tools for semantic mapping research. 


\section{Planning Informative Drone Surveys}

In many forestry applications, the region of interest is commonly substantially larger than what is feasible to survey, either by hand or even with a drone. In practice, foresters  select a small set of plots to visit and extrapolate from these sparse observations to the entire region. These plots are chosen using expert knowledge of the region to be diverse and representative. Despite the ability of drones to cover much larger regions than humans alone, they still fall short of the ability to perform  exhaustive coverage. Therefore, it is clear that judicious use of limited resources is critical.

%Satellite or aerial data is the primary modality of data that scales to the extent required for understanding forests at scale. Unfortunately, prior work has shown that many existing satellite prediction models are highly inaccurate \cite{}. Furthermore, useful global models exist only for select satellite data product and for common tasks like biomass estimation. Therefore, they are not applicable when improved satellites are launched or if a regional satellite or aerial campaign is conducted. %It is possible to tackle fine-grained tasks such as species classification with satellite data, but they are often only relevant to a small region \cite{Sweden}. 
%Therefore, we wish to streamline the process of developing satellite prediction models by using a generic framework and fitting it to local observations. %

Specifically, we assume that our drone collects data that can be accurately interpreted to predict the quantity of interest. For example, we can task a human annotator with identifying tree species from high-resolution drone images, or train a deep learning model to do the same. We further assume that information relevant to this task is contained in remote sensing data of the scene, but it's not easy or scalable to label this data directly. This may be because a human annotator does not possess the intuition to label species based solely on low-resolution satellite data. This claim is especially relevant if the remote sensing data contains channels other than Red-Green-Blue, as it is hard for humans to fully interpret this data 

%\cite{Hard to label multispectral}.
Given these assumptions, the goal is to choose a set of sample locations to observe with the drone. From these observations, we obtain a classification label for each observed pixel in the remote sensing data. We then train a satellite prediction system using these observed pixels as the ground truth training examples. The goal of this work is to observe regions that serve as useful training samples for this satellite prediction system. 
%This is similar to the ideas of active learning \cite{Ren2022ALearning}, where a repository of unlabeled data is available and an algorithm can query an oracle for labels of a subset of samples. However, active learning is not directly applicable to our domain since in classical formulations, any combination of samples can be selected. In this work, the samples relate to spatial regions that must be visited by a drone. Therefore, it must be possible to visit all the requested samples subject to operational constraints such as the drone's battery life.
This problem is most closely related to prior work in informative path planning (IPP), which was first explored by Binney et al. \cite{Binney2013OptimizingPhenomena}. This diverse field is concerned with how and where to sample observations with an agent to gain some understanding of phenomena of interest. Due to the diversity of application domains that have their own specific objectives, assumptions, and constraints, there are a diversity of approaches to tackling this broad problem. In our domain, an important consideration is that the entire mission must be planned before takeoff because commodity drones do not have the computation or flexibility to re-plan the mission in- light.
%For our domain, a desirable informative path planning algorithm:

%\begin{itemize}
%    \item \textbf{Considers non-spatial features:} Since we assume that we have prior satellite or low-resolution aerial data of a scene, it is important that an intelligent algorithm leverages this information.
%    \item \textbf{Plans over a long horizon:} The commodity drones considered in this work must execute a plan developed prior to launch, e.g. offline, and cannot replan in-flight.
%    Therefore, it is important that the algorithm can produce a plan which can be executed for many observations sequentially, ideally as many as can be taken before the battery must be replaced. 
%    \item \textbf{Is computationally fast and scalable:} We desire that this algorithm can be executed on a laptop in the field to enable easy deployment.
%    To be operationally useful, it must be possible to develop a plan for a new---potentially large---region in a short period of time.
%    \item \textbf{Considers area measurements:} Many informative path planning algorithms assume that an observation is a single point in space. Instead, we want to plan were to observe with a downward-facing camera or where to survey a small region. In either case, these observations cover a region that is too large to be approximated as a single point.
%\end{itemize}


\subsection{Offline Methods}
%\begin{itemize}
%    \item \textbf{Ergodic} Ananya's sparse sensing \cite{Rao}
%    \item \textbf{Generic} Sankalp's \cite{Arora2017RandomizedConstraints}, Daniela Rus' Correlated orienteering \cite{Yu2016CorrelatedTasks}
%    \item \textbf{TIGRIS} \cite{Moon2022TIGRIS:Planning}
%    In this work the authors use a sampling based approach. The chance of taking a sample is biased toward those locations which have higher information content. Importantly, this work takes into account the s
%\end{itemize}
%
A number of existing works tackle the problem of offline informative planning. One class of approaches use ergodic planning, which was introduced by \cite{ergodic2011}. These approaches seek to optimize an agent's trajectory over a \textit{information map}---which represents how interesting each sample is---subject to the path length and kinematic constraints. Specifically, the goal is to match the spatial time-averaged statistics of the observations with the distribution of the information match, when compared using a Fourier representation. This approach seeks to strike a balance between exploration and exploitation, by promoting exploration in some areas with a low information value.
A recent extension of this work is sparse-sensing ergodic planning \cite{Rao}, which seeks to optimize the location of 
a limited set of observations, rather than assuming that observations will be collected along the entire trajectory. Ergodic approaches can be applied in our context, but they only optimize for a spatially-representative set of observations and do not consider other features. Since we assume access to prior imagery, we expect that better performance can be achieved by observing a representative set of these appearance features. 

One work that specifically focuses on planning an informative drone flight is TIGRIS \cite{Moon2022TIGRIS:Planning}. This approach also takes as input an information map, which may be overlaid on a non-flat geometry. It is designed for a fixed-wing drone with a forward-facing camera. Using the parameters of the camera, the planner uses Monte Carlo tree search \cite{Browne2012AMethodsEdited} to build a plan that tries to observe areas of high information while not being redundant. Since this approach is designed for target search, it does not prioritize any notion of diversity but simply tries to observe the most highly-informative areas under the dynamic constraints of the drone. 

An approach that is very similar to our work is RIG-Tree \cite{Hollinger2014Sampling-basedAlgorithms}. This is a sampling-based approach that uses a branch-and-bound formulation to maintain tractability. One limitation of this work is it does not have an elegant method to deal with a goal state. Instead, the algorithm only precludes states that would not leave sufficient budget to reach the goal. This means there is less exploration effort put toward the end of the path, even though it is as important as the beginning.

%Talk about \cite{GhaffariJadidi2019Sampling-basedMonitoring}.

\subsection{Online Methods}
Significant work has been done under the assumption that the agent can re-plan its trajectory online, based on the observations it sees during execution. This requires a sophisticated robotic platform that can sense its environment, process this data into a useful format, re-plan which regions would be useful to explore, and execute this plan. In most cases, the agent maintains some sort of belief of which regions are uncertain, desirable, or a combination of both. At each re-planning iteration, the agent seeks to develop a plan that is likely to optimize its objectives within the operational constraints such as path length.

The work of \cite{Popovic2020} proposes a generic framework for terrain monitoring with UAVs equipped with a downward-facing camera. This work assumes that the quantity of interest is a scalar field defined over the environment that has spatial correlations, e.g. similar locations will have similar values. The example use-case is modeling the density of weeds in an agricultural application, where it is most important to accurately model regions with a high infestation. Further, it assumes that the UAV is able to re-plan its trajectory in flight, after taking uncertain measurements of the environment. A strength of this work is that it models the altitude-dependent effects of a drone camera: at higher altitudes, the camera can observe more area but the quality of the measurements will be lower. Using an optimization-based framework and a probabilistic sensor model, the algorithm plans a sequence of observations that decrease the uncertainty while focusing on regions of high predicted weed density. This first part of this path is followed until a new plan based on new observations is developed. This approach is shown to be more effective than a uniform coverage plan at a fixed altitude because it can predict which regions will have more weeds and spend more of the sensing budget there. An extension of this work \cite{Stache2021AdaptiveSegmentation} further explores these concepts with real data. Specifically, they use semantic segmentation to identify weeds in the images and fit empirical models to the segmentation accuracy at different altitudes. This work also shows higher accuracy than a non-adaptive baseline. In both cases, a fundamental strength of the approach is the reason we cannot use it in our domain---online re-planning based on the observed data is critical to the performance increase over the baseline.

A key feature of the previous work is that it seeks to model only regions that were observed with the drone. The main decision is how many times to re-observe a region and at what altitude. In our setting, we wish to make observations with a drone and extrapolate beyond them with satellite data. This goal is somewhat related to that of \cite{Ruckin2022}, which seeks to plan a drone flight that collects images to train a deep learning model for land use classification. In this setting, it is assumed that the drone has a downward-facing camera that observes a scene. It also possesses a deep learning model that predicts the land use for each pixel, as well as a measure of uncertainty about the prediction. The goal is to collect images, that when labeled by a human annotator after the mission, can be used to update the model the deep learning model so its performance is better on new data. The authors show that collecting images that the current model is uncertain about leads to better performance after retraining. Therefore, when the drone is collecting data, it should prioritize images that the current model is uncertain about. As the drone explores a region, it generates the uncertainty predictions for each observed image, but cannot access the true label. Then, it continuously updates the plan that tries to observe new uncertain images, by assuming that unobserved images next to uncertain observed images will also be uncertain. As with the prior work, this approach requires online reasoning to be effective. Furthermore, this approach has a subtly different goal from ours: their goal is to collect images that can be used to train a good model for new drone observations whereas our approach assumes a model exists already for drone data and these predictions can be used to inform a satellite model. 

Some prior work formulates the problem in a similar way to our objective. Work by Kodule et. al. \cite{Kodgule2019Non-myopicMeasurements} and an extension by Candela et. al. \cite{Candela2020PlanetaryMapping} explore the problem of where to gather information with a planetary robot, e.g. Mars Rover, given that the whole region has already been observed by a low-fidelity orbiting satellite. Specifically, it is assumed that the world is represented as a grid of pixels each containing a multispectral (8-channel) observation. The goal of this work is to predict hyperspectral data for the entire region using only sparse hyperspectral observations from the agent and the multi-spectral data available everywhere. To accomplish this, multiple Gaussian Processes \cite{Rasmussen2004} are defined which uses the spectral values at each pixel as well as the spatial location of each pixel as features. Then the agent uses Monte Carlo Tree Search (MCTS) \cite{Browne2012AMethods} to plan a set of sampling locations in the environment that decreases the uncertainty of the Gaussian Process. The agent then moves to the next sample on its path, takes a hyperspectral measurement, and replans its future trajectory. This approach is conceptually-similar to our objective, except that it is designed for online reasoning and is computationally demanding due to the MCTS.

A similar problem is studied by Edelson et. al. \cite{Edelson2020ErgodicGathering}, which replaces the MCTS planner with an Ergodic planner. While ergodic planning is an offline approach, this work proposes to re-plan an ergodic trajectory online after a number of \textit{in-situ} samples have been collected. After this time, the uncertainty map is updated based on these new observations, and a new trajectory is planned. This can be seen as a way to prioritize collecting spectrally-diverse samples because the information map will show that samples that are spectrally similar to those collected already have low uncertainty. Therefore, these well-modeled locations will not be prioritized in planning. 

The goal of this work is to bridge the gap between these two classes of approaches. Specifically, we wish to develop a planer that can function offline, while still retaining much of the strong performance of an online planner.
%Due to the assumption of prior imagery, we may leverage approaches from \cite{Candela2021}, which uses an MCTS planner to explore a world for which prior satellite data exists. This work uses a Gaussian Process to model a quantity of interest. It is challenging to directly apply this work because it assumes that each pixel can be regressed individually by the Gaussian Process. This assumption is unlikely to be correct for high spatial and low spectral resolution data since texture within a local neighborhood will likely need to be considered.

%that can be used to gather information to train a good model for satellite data
%Alberto's Planetary , Alberto's Coral \cite{Candela2021}
%Yogi's Coral \cite{Jamieson2020ActiveEnvironments},




%\begin{table}[]
%\resizebox{\columnwidth}{!}{%
%\begin{tabular}{|l|l|l|l|l|l|}
%\hline
% & \makecell{Sparse \\ Ergodic \\ Sensing \cite{Rabbi2020Small-ObjectNetwork}} & \makecell{Optimization-\\based \\ replanning \cite{Popovic2017OnlineUAVs}} & TIGRIS \cite{Moon2022TIGRIS:Planning} & \makecell{MCTS on \\ GPs \cite{Candela2020PlanetaryMapping}} & Proposed \\ \hline
%\makecell{Non-spatial \\ features} & X & X & X & Y & Y \\ \hline
%\makecell{Long horizon} & Y & X & Y & Y & Y \\ \hline
%\makecell{Fast \& scalable} & Y & Y & Y & X & Y \\ \hline
%\makecell{Area \\ measurements} & X & Y & Y & Y & Y \\ \hline
%%\makecell{Incorporates \\ non-trivial \\ dynamics} & Y & Y & Y & N & N \\ \hline
%\end{tabular}%
%}
%\caption{A comparison of the capabilities of existing informative path planning algorithms.}
%\label{tab:my-table}
%\end{table}
%
%We can apply concepts from online approaches to offline approaches
%We can close the gap between online and offline approaches 

