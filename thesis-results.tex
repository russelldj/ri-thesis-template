\chapter{Results} \label{chapResults}
\section{Photogrametry on Drone Forest Images}
An important first step in multiple approaches we used is understanding the structure of the forest scene. As one solution to this problem, we ran Agisoft Metashape with the parameters from Young et al. \cite{Young2022} on a variety of datasets. Three datasets shown in Figure \ref{fig:results:sfm} are representative of the types of data we experimented with. The first dataset, \textit{Stowe}, was collected with a lawnmower survey from a commodity drone. The locations of all the cameras were properly estimated and the results look good throughout. Some geometry of the trees can be seen, but much of the fine detail is lost, which is common for photogrammetry. The next dataset is \textit{Warton}, which was collected with the custom payload at an off-nadir angle of 60 degrees from horizontal and manual flight pattern observing roughly the same region from different angles. All of the cameras were properly aligned, but the quality of the mesh varies dramatically. In the center where there are multiple observations, the results are fairly good. However, both the structure and texture become blurrier farther away from the central area. This is likely because there are fewer camera views to triangulate and those regions are farther away. This mesh is darker than the \textit{Stowe} it was underexposed, due to a fixed exposure between collects. It is important to balance under- and over-exposure, especially in regions with different brightness. A further consideration is that longer exposures are more prone to motion blur, which is especially common while the drone is turning.
Finally, \textit{Coimbra} is a dataset where the drone follows a path between trees. The data is captured with the custom payload facing forward at 30 degrees from horizontal. A large portion of this trajectory appeared to be roughly accurate. However, in multiple instances, a set of cameras and the associated mesh were rotated relative to the correct orientation. This resulted in odd artifacts that made the mesh effectively unusable. Furthermore, the level of visual detail was fairly low.


\begin{figure}[H]
    \subfloat{\includegraphics[width=0.45\textwidth]{figs/results/geometric_understanding/stowe_anew_collect_000_zoomed_out.png}}
    \hfill
    \subfloat{\includegraphics[width=0.45\textwidth]{figs/results/geometric_understanding/stow_anew_collect_000_zoomed_in.png}}\\
        
    \subfloat{\includegraphics[width=0.45\textwidth]{figs/results/geometric_understanding/burn_zoomed_out.png}}
    \hfill
    \subfloat{\includegraphics[width=0.45\textwidth]{figs/results/geometric_understanding/burn_zoomed_in.png}}\\
    
    \subfloat{\includegraphics[width=0.45\textwidth]{figs/results/geometric_understanding/coimbra_zoomed_out.png}}
    \hfill
    \subfloat{\includegraphics[width=0.45\textwidth]{figs/results/geometric_understanding/coimbra_zoomed_in.png}}
    \caption{3D reconstructions using Agisoft Metashape on three different environments. From top to bottom, they are a lawnmower pattern with a commodity drone,  The full map is shown to the left and a zoomed-in inset is shown to the right.}
    \label{fig:results:sfm}
\end{figure}

These results support the common practice of automated drone surveys using a lawnmower pattern since this method yielded consistently high-quality results across all the datasets we experimented on. However, it also shows that manual flight patterns over the canopy yield some useful data and challenging under-canopy conditions are somewhat successful. This suggests that traditional drone surveys are well-motivated, but further research on photogrammetry for more challenging forest scenarios may yield useful results.

%A practical consideration when working with these systems is the camera exposure. This is generally handled well by commodity systems, but may need to be hand-tuned on an experimental robotic system. If the exposure is too long, especially in the case of rapid drone movement, there can be motion blur which degrades the quality of reconstructions. Similarlly, both over and under-exposed images lead to fewer features to match between images and areas of minimial information in the final mesh.  

\section{Simultanous Localization and Mapping in Forest Environments}

\begin{figure}[H]
   \centering
   %----primera subfigura----
   \subfloat[]{
        \includegraphics[width=0.465\textwidth]{figs/results/geometric_understanding/metashape_cloud_v3.png}}
   %\hspace{0.1\linewidth}
   \subfloat[]{
        \includegraphics[width=0.5\textwidth]{figs/results/geometric_understanding/slam_ptCloud.eps}}\\%\\[20pt]   
   \subfloat[]{
        \includegraphics[width=0.5\textwidth]{figs/results/geometric_understanding/housedorff_distance.eps}}%\\[20pt]            
   \caption{Point cloud maps of the A) photogrammetry baseline, B) SLAM outcome. The two maps were compared using the Hausdorff distance, whose result is visualized as C) the SLAM map colored according to this metric. Note that the baseline and the SLAM clouds color maps correspond to RGB and height values, respectively. Analysis and figure provided by Francisco Yandun.}
   \label{fig:results:slam_photogrametry_comparison}
\end{figure}

We were interested in comparing the results of SLAM to photogrammetry because these two systems represent an interesting trade-off. SLAM approaches generally require multiple synchronized and calibrated sensors, with LiDAR being a common requirement. However, they can run in an online manner which can be used to inform robots' next actions. Photogrammetry requires only images, with optional GPS. The downside is it is computationally expensive and must be run in a batch after all the images have been collected.

We conduct this comparison on the \textit{Gascola} dataset that was surveyed with lawnmower coverage and the custom payload at a slight off-nadir angle. We ran photogrammetry using the default parameters. Francisco Yandun ran a tuned version of LIO-SAM \cite{Shan2020LIO-SAM:Mapping} and conducted a comparison between the two approaches.

As shown in Figure \ref{fig:results:slam_photogrametry_comparison}, the two maps agree fairly well in most regions. A notable source of high error is the tops of trees. In general, our qualitative assessment suggested that the SLAM approach was better at capturing these fine details compared to photogrammetry. This is because it had the benefit of explicit 3D data from LiDAR, while the photogrammetry approach had to infer 3D information from images. This often leads to sharp points and fine detail being lost. This shows that in offline applications of over-canopy mapping, photogrammetry is likely sufficient. 

\section{Tree Detection using Data at Multiple Scales}
The goal of these experiments is to provide insight into the performance of DeepForest in different settings, with the eventual goal of informing what data would be sufficient for useful tree detection.

The first aspect we explore is the influence of the image resolution that is provided to the detection network. The authors of DeepForest state that this parameter is important, but to the best of our knowledge do not provide any experimental analysis of its impact. The first situation this is useful in is when high-resolution data is available. Then, these experiments can inform what resolution to downsample to for best performance. The second is when planning drone data collection or conducting post-processing. As long as the resolution is suitable for photogrammetry, there is no reason to collect data that is higher resolution than is needed for network inference. Similarly, there is no reason to export an orthomosaic that is higher resolution than needed, since this can often result in very large images. 



To study this phenomenon, we use data from both the drone orthomosaic and NAIP. For each type of data, we conducted experiments with both the DeepForest model trained on NEON data, termed \textit{pretrained} and a model that was finetuned on one half of the data from the site, termed \textit{finetuned}. In both experiments, we trained the network for 50 epochs using the default settings of the Adam \cite{} optimizer provided by DeepForest. The confidence threshold at inference was set to 0.3, which is also the recommended default. The predictions are evaluated on the test set, which is spatially distinct from the training set. The experiment is repeated with 2-fold validation, using one half as the training set and other half as the test set. To provide increased confidence in the results, each configuration is repeated three times. These six trials are repeated for 20 different inference resolutions sampled logarithmically from 0.01 to 0.7 meters per pixel. In the \textit{finetuned} experiments, the training resolution is the same as the inference resolution.

A common metric to assess the quality of a detection prediction is intersection over union (IoU). This is the ratio of the area of the overlap between the prediction and groundtruth to the total area covered by both. One metric we use to assess the quality of the predictions is the mean intersection over union (mIoU), which is the IoU with the closest groundtruth tree, averaged over all predictions. We also compute precision and recall. 
The authors of DeepForest state that a predicted tree with an intersection-over-union (IoU) of 0.4 with a groundtruth tree, is useful for ecological applications. A true positive \textit{TP} is when a predicted tree has sufficient overlap with a ground truth. A false positive \textit{FP} is a spurious detection when a tree is predicted but does not have sufficient overlap with any ground truth tree. Finally, a false negative \textit{FN} is a missed detection where no prediction overlaps with a given ground truth tree. Precision represents the fraction of predictions that match a ground truth and recall represents the fraction of groundtruth trees that are matched by a prediction. These equations are summarized below:

\begin{align*}
\textrm{Precision} &= \frac{\textrm{TP}}{\textrm{TP}+\textrm{FP}}\\
\textrm{Recall} &= \frac{\textrm{TP}}{\textrm{TP}+\textrm{FN}}
\end{align*}

\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{figs/results/tree_detections/drone_metrics.pdf}
    \includegraphics[width=0.45\textwidth]{figs/results/tree_detections/NAIP_metrics.pdf}
    \caption{Tree detection metrics for drone data and NAIP versus inference resolution.}
    \label{fig:results:inference_resolution}
\end{figure}

The results of these experiments are presented in Figure \ref{fig:results:inference_resolution}. The pretrained model on the drone data exhibits consistent performance across all metrics between 0.01 and 0.1 meters per pixel. As the resolution becomes coarser than 0.1 meters, the performance quickly deteriorates. With the finetuned model, performance remains much more consistent across resolutions from 0.01 to 0.7 meters per pixel. This suggests that low-resolution data is too far out of distribution for the pretrained network, but the finetuning allows it to still perform well even on data that is different from what was seen during the NEON training. The NAIP data shows a different trend. For both the pretrained and finetuned models, the performance is consistent but low for all resolutions. The exception is for the pretrained model, the mIoU drops, and the precision increases for the coarsest data.

Because of the poor performance of NAIP data, we wanted to explore other options for remote sensing data. Because it is challenging to obtain higher-quality, commercial remote sensing data, we instead chose to simulate varying resolutions of data by downsampling the orthomosaic to simulate lower-resolution remote sensing products. This has the added benefit of removing confounding effects from different data products and focusing solely on resolution. Given our previous experiments, we chose to perform inference at a resolution of 0.1 meters per pixel. Therefore, we simulated data that was lower resolution than this threshold by downsampling the orthomosaic to resolutions between 0.1 and 1 meters per pixel, sampled logarithmically. 

\begin{figure}
    \centering
    \includegraphics[width=0.6\textwidth]{figs/results/tree_detections/simulated_RS_metrics.pdf}
    \caption{Performance of tree detection with downsampled drone data, used to simulate remote sensing data of different resolutions.}
    \label{fig:results:downsampled_tree_det_resolution}
\end{figure}

The results of this experiment are presented in Figure \ref{fig:results:downsampled_tree_det_resolution}. For an input resolution of 0.1 meters per pixel, the performance of the pretrained and finetuned models are similar. As the resolution decreases, the performance of the pretrained model drops dramatically. At approximately 0.3 meters per pixel, the performance of the pretrained model begins to decrease less rapidly. In contrast, the performance of the finetuned model only decreases slightly as the resolution drops. At a resolution of 0.6, the performance of the downsampled orthomosaic is still significantly better than that of the NAIP data. This suggests that despite the comparable resolution, something about the orthomosaic is more suitable for tree detection with DeepForest. One potential explaination is the difference in colors between the two types of data.

Finally, we propose  propose a set of experiments to explore different approaches to using multiple types of data. The baseline approach is applying the pre-trained DeepForest model to the aerial data. This requires no field measurements but is expected to produce low-quality detections because the model was trained on drone data at native resolution, which has significantly more texture. The next approach is using the pre-trained version of DeepForest on the drone data. This is expected to be significantly better than the aerial data because it is similar to what the model was trained on. The next two experiments involve fine-tuning the drone and aerial models on the small set of ground truth measurements. We expect that this fine-tuning will have a positive impact on the performance. Finally, we train a model for remote sensing data on the predictions from the drone. Even though these predictions are less accurate than manual labels, there are more of them, so this may improve performance.

In all experiments, the training and inference resolutions are 0.1 meters.

\begin{itemize}
    \item \textbf{Pretrained ortho} This is applying the DeepForest model trained on NEON data to the drone orthomosaic. To deploy this approach, only drone data is needed.
    \item \textbf{Finetuned ortho} This is fine-tuning the DeepForest model trained on NEON data with a small amount of site-specific data. This requires both drone data and labels from field work or manual annotations.
    \item \textbf{Pretrained NAIP} This is applying the DeepForest model trained on NEON data to NAIP. This requires only NAIP data.
    \item \textbf{Finetuned NAIP} This is fine-tuning the DeepForest model pretrained on NEON data with a small ammount of site-specific data. Similar to \textit{finetuned ortho}, this requires two types of data, NAIP and a small ammount of tree labels.
    \item \textbf{Finetuned NAIP on predictions from pretrained ortho (Finetuned NAIP[pretrained])} 
    This a model for NAIP data. Instead of finetuning this model on labels taken from field work or manual annotations, this model is finetuned using trees predicted by the \textit{pretrained ortho} model using drone data. This experiment does not require labeled data and only needs drone and NAIP data. The hypothesis is that the increased number of training samples can compensate for the decreased quality obtained by training on automated predictions.
    \item \textbf{Finetuned NAIP on predictions from finetuned ortho (Finetuned NAIP[finetuned])}
    This is also a model for NAIP data. Similar to the previous experiment, the model for NAIP is finetuned using predictions on drone data. In this case, the predictions are generated using the \textit{finetuned ortho} model. This means that all three types of data are required: labeled data, drone data, and NAIP.
    \item \textbf{Finetuned downsampled ortho on predictions from pretrained ortho (Finetuned DS ortho[pretrained])}
    To simulate remote sensing data that is more useful for tree detection than NAIP, we downsample the orthomosaic to 0.6 meters per pixel. Then we conduct the same experiment as \textit{finetuned NAIP[pretrained]}.
    \item \textbf{Finetuned downsampled ortho on predictions from finetuned ortho (Finetuned DS ortho[finetuned])} This experiment is also conducted with a downsampled orthomosaic used to simulate remote sensing data. Otherwise, it is the same as \textit{finetuned NAIP[finetuned]}.
    
\end{itemize}

In this experiment we generate predictions on two different modalities from the same \textit{Stowe} cite. The first is an orthomosaic derived from the drone and the second is NAIP imagery derived from aerial data. These predictions are shown in Figure \ref{fig:results:tree_det}. The predictions on drone data appear significantly better than those from the NAIP data. This is unsurprising because of the increased detail provided by the higher-resolution data. What is somewhat surprising is that fine-tuning the model did not appear to improve performance significantly. This suggests that the pretrained model already is already well-suited to this data, especially after appropriately matching the training resolution.

In Table \ref{tab:results:tree_det} we show the quantitative evaluation of the different approaches. We use precision, recall, and average intersection over union (IoU) to evaluate the results. The IoU represents the ratio of the overlap between a predicted and ground truth rectangle and the union of these two rectangles. A score of 0 means there is no overlap while a score of 1 means the prediction is perfect. The authors of DeepForest state that in ecological applications, an IoU of 0.4 is sufficient to be useful. Therefore, the precision metric represents the fraction of predicted trees that overlap with an IoU of at least 0.4 with a ground truth tree. The recall is the fraction of ground truth trees where a predicted tree overlaps with at least 0.4 IoU. Finally, the IoU column is the average IoU across all predictions with the best-matching ground truth tree.

\begin{table}[]
    \centering
    \begin{tabular}{|l|l|l|l|}
        \hline
        \textbf{Experiment} & \textbf{Recall} & \textbf{Precision} & \textbf{mIoU}\\
        \hline
        Base ortho                           & $0.614 \pm 0.000$ & $0.601 \pm 0.076$ & $0.424 \pm 0.007$ \\ \hline 
        Finetuned ortho                      & $\mathbf{0.620} \pm 0.026$ & $\mathbf{0.608} \pm 0.079$ & $0.440 \pm 0.021$ \\ \hline 
        Base NAIP                            & $0.125 \pm 0.004$ & $0.100 \pm 0.009$ & $0.202 \pm 0.004$ \\ \hline 
        Finetune NAIP                        & $0.292 \pm 0.030$ & $0.242 \pm 0.017$ & $0.261 \pm 0.018$ \\ \hline 
        Finetune NAIP on ortho base          & $0.326 \pm 0.052$ & $0.289 \pm 0.034$ & $0.279 \pm 0.026$ \\ \hline 
        Finetune NAIP on ortho finetuned     & $0.319 \pm 0.051$ & $0.316 \pm 0.025$ & $0.262 \pm 0.029$ \\ \hline 
        Base DS ortho                        & $0.303 \pm 0.010$ & $0.231 \pm 0.033$ & $0.266 \pm 0.014$ \\ \hline 
        Finetune DS ortho                    & $0.576 \pm 0.034$ & $0.563 \pm 0.068$ & $0.412 \pm 0.033$ \\ \hline 
        Finetune DS ortho on ortho base      & $0.612 \pm 0.015$ & $0.563 \pm 0.076$ & $0.420 \pm 0.013$ \\ \hline 
        Finetune DS ortho on ortho finetuned & $0.615 \pm 0.024$ & $0.546 \pm 0.076$ & $\mathbf{0.444} \pm 0.025$ \\ \hline 
    \end{tabular}
    \caption{Tree detection results using multiple experimental strategies. The approaches are evaluated on precision and recall at an IoU threshold of 0.4 and the average IoU for all predictions.}
    \label{tab:results:tree_det}
\end{table}

These experiments present several key takeaways. As expected, drone data provides the best prediction results and is significantly better than remote sensing data. Fine-tuning the model on local data helped marginally in the case of drone data but more significantly in the case of NAIP data. This is understandable because the DeepForest model was pretrained on drone data from representative environments but not NAIP data. Finally, a model for NAIP data trained on drone predictions was better than a pre-trained model but worse than a model trained solely on a small set of annotations. This suggests that the errors in the drone predictions propagate into even larger errors in the remote sensing predictions. Overall, these approaches show that predictions from drone data are likely to be useful in a variety of ecological applications, but NAIP cannot be used for accurate tree detection using the current methods. However, it's possible that even these low-quality predictions could be used for some tasks such as estimating the number of trees or registering datasets by using trees as landmarks.

\begin{figure}[H]
    \subfloat[Pretrained ortho]{\includegraphics[width=0.45\textwidth]{figs/results/tree_detections/deepforest_vis/ortho_base_preds.png}}
    \hfill
    \subfloat[Finetuned ortho]{\includegraphics[width=0.45\textwidth]{figs/results/tree_detections/deepforest_vis/ortho_finetuned_preds.png}}
    \vfill
    \subfloat[Pretrained downsampled ortho]{\includegraphics[width=0.45\textwidth]{figs/results/tree_detections/deepforest_vis/simulated_RS/RS_base_preds.png}
    }
    \hfill
    \subfloat[Finetuned downsampled ortho]{\includegraphics[width=0.45\textwidth]{figs/results/tree_detections/deepforest_vis/simulated_RS/RS_finetuned_preds.png}}
    \vfill
    \subfloat[Pretrained NAIP]{\includegraphics[width=0.45\textwidth]{figs/results/tree_detections/deepforest_vis/RS_base_preds.png}}
    \hfill
    \subfloat[Finetuned NAIP]{\includegraphics[width=0.45\textwidth]{figs/results/tree_detections/deepforest_vis/RS_finetuned_preds.png}}
    \caption{
    Tree detections on the test set for drone orthomosaic, downsampled orthomosaic, and NAIP data. The left column is the pretrained model while the left is finetuned with site-specfic data. Predictions are in blue and groundtruth is in gold. Best viewed zoomed in.
    }
    \label{fig:results:tree_det}
\end{figure}


\section{Semantic Mapping for Vegetation Classification}
The goal of this work is to build a 3D map of the environment that is annotated with the type of vegetation at each location. A challenge of this work is that we did not have access to field-reference data for the true locations of the different types of vegetation. Therefore, we conduct much of our quantitative evaluation on the semantic segmentation predictions, which can be more easily compared to human-labeled images. 

\subsection{Image Segmentation}
We conduct two experiments related to semantic segmentation. The first is a study using under-canopy data to determine the potential for synthetic pretraining and the impact of the number of training images on performance. The second is an evaluation of the model we used in over-canopy mapping with a small set of training examples.
\subsubsection{Sete Fonte Experiment}

\begin{figure}
   % First row
   \centering
    \includegraphics[width=\linewidth]{figs/results/semantic_segmentation/SeteFontes/qualatative_001400.png}  
    \includegraphics[width=\linewidth]{figs/results/semantic_segmentation/SeteFontes/qualatative_001600.png}  
    \caption{Predictions on the \textit{Oporto} dataset. Black is background, red is fuel, brown is trunks, and green is canopy.
    %Note that low trees are often predicted as fuel.
    } 
    \label{fig:results:oporto_semantic_seg_qual}
\end{figure}

Given the limited availability of real data and the labor-intensive nature of labeling to obtain ground truth, we explored the utility of models trained with simulated (\textit{synthetic}) data. We conducted three types of experiments: models trained solely with \textit{synthetic} data, models trained with real data (\textit{Setes Fontes}), and a mixture of both. For the last two cases, we trained different models with a variable number of real images to evaluate the performance of the model as the number of real labeled images varied. Thus, we conducted training experiments using (or adding) 7, 15, 21, 30, 60, 91, and 121 data points from the \textit{Setes Fontes} dataset. We trained for 10000 iterations and evaluated each model on 30 \textit{Setes Fontes} images not seen in the largest training split. We replicate this experiment over five folds of the data. For all three models the base networks were first pretrained with the \textit{CityScapes} dataset \cite{Cordts2016}. The mean Intersection over Union on the test set of each of these configurations is shown in Figure \ref{fig:results:semantic-size-pretraining}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{figs/results/semantic_segmentation/synthetic_experiments_mious.pdf}
    \caption{Test mIoU for very few training images on the \textit{Setes Fontes} dataset.
    Error bars represent minimum and maximum results across the five folds of \textit{Setes Fontes}.}
    \label{fig:results:semantic-size-pretraining}
\end{figure}
It is interesting to note the relatively high performance of a model that used only 7 real images. Also, the model trained solely on synthetic data fails to generalize to real data, even after properly accounting for differences in mean and variance of both datasets. We found that combined real and synthetic data performs worse than training the same model only using real data. This suggests that the synthetic data comes from a completely different distribution than the real one, making its contribution detrimental. In the future, we plan to keep researching the causes of this interesting outcome. It is possible that simple attributes such as saturation or spatial resolution is the cause of this domain shift. Alternatively, it's possible that significantly more effort needs to be put into realistic simulations for them to be useful to train prediction models.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\columnwidth]{figs/results/semantic_segmentation/confusion_matrix.pdf} 
    \caption{Confusion matrix for the \textit{Setes Fontes} test datasets normalized per class with the true fraction of each class reported on the y axis labels.
    %Fuel is predicted fairly well, though it is fairly often confused for canopy, which is understandable given the visual similarity and the large fraction of canopy pixels.
    }
    \label{fig:results:sete_fontes_confusion_matrix}
\end{figure}

Since the model trained in 121 images ($80\%$ of the \textit{Setes Fontes} dataset) showed the best performance in these experiments, we conducted further experiments on it. The confusion matrix on the \textit{Sete Fontes} test set is shown in Figure \ref{fig:results:sete_fontes_confusion_matrix}. This shows that all four classes are predicted with reasonable accuracy. A common source of confusion is between trunks and canopies, which is understandable because they frequently overlap. Canopies are also confused for background, which includes the sky. Even though these two classes look very different, the sky can often be seen through the canopy, and the network miss-classifies these fine details. In most cases, this error is harmless because no LiDAR information should correspond to the sky pixels. Finally, the most common error for fuel is canopies, which is understandable because they are both leafy vegetation. Information about the height, either provided to the model or used in post-processing, could help disambiguate this confusion. 

Since we are mainly interested in the fuel instances, we aggregated the background, trunks, and canopies in a single non-fuel class. In that case, we obtained an IoU of 78.2\% and 95.3\% for \texttt{Fuel} and \texttt{Not Fuel}, respectively, which yields a mIoU of 86.7\%. This shows that our system performs well at its primary task of identifying fuel.

The end goal of this model is to be useful on the semantic mapping task on the \textit{Oporto} dataset, which wasn't seen during training. We show two qualitative examples of predictions in Figure \ref{fig:results:oporto_semantic_seg_qual}. The predictions still appear fairly accurate, despite the change in camera perspective from ground to aerial and different image characteristics. 


\subsubsection{Gascola Experiment}
In this experiment, we used two different datasets from \textit{Gascola} which partially covered the same region. We train on one dataset and test the model on the other. Because of the small number of images that we used in this experiment, these two datasets had different class distributions as shown in Figure \ref{fig:results:semantic_class_fracs}, though they had the same three main classes. Each of these three classes corresponded to a different aggregate class (Background, Canopy, and Fuel), so it was important to effectively tell them apart. 

\begin{figure*}[h!]
   \centering
   %----primera subfigura----
   \subfloat[]{
        \label{fig:train_class_frac}         %% Etiqueta para la primera subfigura
        \includegraphics[width=0.5\textwidth]{figs/results/semantic_segmentation/class_fraction_train.png}}
   %\hspace{0.1\linewidth}
   \subfloat[]{
        \label{fig:test_class_frac}         %% Etiqueta para la segunda subfigura
        \includegraphics[width=0.5\textwidth]{figs/results/semantic_segmentation/class_fraction_test.png}}%\\[20pt]            
   \caption{This shows the fraction of pixels per class for A) the train set and B) the test set. Note that the three dominant classes Canopy, Bare Earth, and Dry Grass are common across both collections but the comparative frequencies are somewhat different. These correspond to our aggregate Canopy, Background, and Fuel classes respectively. The fractions of other classes are fairly small, and some from the training set are entirely absent in the test set.}
   \label{fig:results:semantic_class_fracs}                %% Etiqueta para la figura entera
\end{figure*}

The quality of predictions is shown in Table \ref{tab:results:semantic_eval}.  We summarized the IoU, precision, and recall for each class. The performance is fairly good on the most common classes, but, as expected drops on the rarer classes. As seen by the qualitative examples in Figure \ref{fig:results:semantic_gascola_qualitative}, there are instances where the predictions on the granular classes are incorrect, but the aggregate class is correct.

\begin{table}[ht!]
    \centering
\begin{tabular}{ccccc}
% \hline
\toprule
\textbf{Class} & \textbf{IoU} & \textbf{Precision} & \textbf{Recall} \\ \midrule
% \hline
Canopy & 70.05 & 84.77 & 80.13 \\
Dry Grass & 79.7 & 93.75 & 84.17 \\
Bare Earth & 78.53 & 88.12 & 87.83 \\
Green Shrubs & 3.27 & 21.72 & 3.71 \\
Green Grass & 0.0 & 0.0 & 0.0 \\
Dry Shrubs & 0.0 & 0.0 & 0.0 \\
Live Trunks & 0.05 & 0.05 & 84.09 \\
\bottomrule
% \hline
\end{tabular}
\caption{Evaluation results of the SegNext \cite{Guo2022SegNeXt:Segmentation} network with the Anderson Fuel Model \cite{anderson1981aids} for semantic segmentation in a forestry environment.}
\label{tab:results:semantic_eval}
\end{table}


\begin{figure*}[h!]
   \centering
   \includegraphics[width=0.63\textwidth]{figs/results/semantic_segmentation/Gascola/safeforest_all_classes_flat.png}
   \includegraphics[width=0.33\textwidth]{figs/results/semantic_segmentation/Gascola/safeforest_classmap_compressed_flat.png}
   \includegraphics[width=\textwidth]{figs/results/semantic_segmentation/Gascola/000000_rgb_img.png}
   \vspace{0pt}
   \includegraphics[width=\textwidth]{figs/results/semantic_segmentation/Gascola/000005_rgb_img.png}
   \vspace{0pt}
   \includegraphics[width=\textwidth]{figs/results/semantic_segmentation/Gascola/000010_rgb_img.png}
   \vspace{0pt}
   \includegraphics[width=\textwidth]{figs/results/semantic_segmentation/Gascola/000015_rgb_img.png}
   \vspace{0pt}
   \caption{
   Qualitative semantic mapping results from the test set. The results are shown both for the predicted classes and the aggregated ones, with colors visualized in the top rows.
   White regions in the ground truth represent areas that were ambiguous to the human annotator. Overall the predictions match the ground truth well and boundaries are well-defined. 
   }
   \label{fig:results:semantic_gascola_qualitative}                %% Etiqueta para la figura entera
\end{figure*}

\subsection{Projecting Segmentation into 3D}
The final goal of semantic mapping is to develop a model of the environment and what class different regions are. To evaluate the feasibility of this, we conduct two experiments using multi-sensor data.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{figs/results/semantic_mapping/semantic_cloud_first_approach.png}
    \caption{Semantic mapping results on the Oporto test site. Fuel is red, trunks are green, canopy is purple, and background is black. White points are unlabeled. A video showing the same scene can be seen at \href{https://drive.google.com/file/d/1zulFi7uEZI8R40l-mDwUEBklhR7Kk2_1/view?usp=sharing}{this link}}
    \label{fig:results:semantic_mapping_original}
\end{figure}

The first experiment is conducted on the \textit{Oporto} dataset, which is collected in a clearing in Portugal with the custom payload at an orientation of 30 degrees from horizontal. For semantic segmentation, we use the model trained on \textit{Sete Fontes}. For localization, we use the vision-LiDAR SLAM system from \cite{RussellUnmannedMitigation}.
The results are shown in Figure \ref{fig:results:semantic_mapping_original}. This shows that the system was able to determine that there was significant fuel at ground level and correctly identify the tree trunks in the environment. The latter is important because trunks could be used as a localization aid in a similar way to SLOAM \cite{Chen2020SLOAM:Inventory}. One issue that we observed was that multiple scans were not perfectly registered with each other. This is in contrast to the final point cloud derived from SLAM, where fine details are captured precisely. Our hypothesis is that because the SLAM system uses a pose-graph \cite{Dellaert2017FactorPerception} formulation, information from loop closures can be used to update the historical pose to make it more accurate. However, the semantic mapping system only uses the most recent SLAM pose estimate, which is likely to be less accurate than the final optimized version.


A limitation of the first experiment was our lack of ground truth data to compare against.
In our second experiment on the \textit{Gascola} dataset, we still did not have field reference data but tried to approximate this as accurately as possible.
We chose to label an orthomosaic derived from a 3D model by hand. We used QGis \cite{QGIS_software}, an open-source software for interacting with geospatial data. We labeled three coarse classes, fuel, canopy, and background, which included bare-earth and other non-flammable material. Trunks were not included because they could almost never be observed in the top-down view from the orthomosaic. This manual process took approximately eight hours to complete and is visualized on the left side of Figure \ref{fig:results:semantic_map_UFO}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.40\textwidth, angle=15]{figs/results/semantic_mapping/labeled_orthomoasaic.png}
    \includegraphics[width=0.45\textwidth]{figs/results/semantic_mapping/segnext_gc5_ufomap.png}
    \caption{Manually-labeled result on the left, results from UFO-Map on the right provided by M. Duda Andrada, using the SegNext \cite{Guo2022SegNeXt:Segmentation} model trained on Gascola Data.}
    \label{fig:results:semantic_map_UFO}
\end{figure}

In this experiment, we used the SegNext model trained on the manually-labeled dataset from the flight that was not used for mapping. We used the localization LIO-SAM \cite{Shan2020LIO-SAM:Mapping} and provided by Franciso Yandun to estimate the pose. In this case, we used UFOMap \cite{Duberg2020UFOMap:Unknown} instead of octomap to aggregate the observations, because of the improved performance. This change was implemented by Duda Andrada and she conducted the experiments reported here. The results of this are shown in Figure \ref{fig:results:semantic_map_UFO}. The overall structure of the scene matches well and small local features are correctly identified. One major source of error is seen in the misregistration of data from adjacent drone flights. This is seen in the vertical offsets that are especially visible on the left side of the map. This may be due to latency in the processing or also because of using the un-refined pose as described previously. The quality of this result suggests that with additional work to properly register the scans, this system could be a powerful tool for vegetation mapping. 

\section{Informative Path Planning for Commodity Drones}

\begin{figure}
    \centering
    \includegraphics[width=0.3\textwidth]{figs/results/ipp/chesapeake/chesapeake_classes.png}\\
    \includegraphics[width=\textwidth, trim={0 16cm 0 16cm}, clip]{figs/results/ipp/4000/paired_qual/coverage.png}
    \includegraphics[width=\textwidth, trim={0 16cm 0 16cm}, clip]{figs/results/ipp/4000/paired_qual/raptors.png}
    \includegraphics[width=\textwidth, trim={0 16cm 0 16cm}, clip]{figs/results/ipp/4000/paired_qual/raptors_rare.png}
    \caption{Visualizations of the coverage, RAPTORS, and RAPTORS\_rare planners from top to bottom. Each color represents an individual flight and they were executed in the following order: blue, orange, green, then red.}
    \label{fig:results:ipp_pairqual}
\end{figure}
The goal of this experiment is to simulate a land cover mapping mission for a region that is too large for the drone to exhaustively survey. Remote sensing data is available beforehand and is used to both inform the mission and generate predictions on the regions that are not observed by the drone. In this experiment, we use NAIP data \cite{U.S.DepartmentofAgriculture2011NationalSheet} and manual land cover classification annotations from the Chesapeake Bay \cite{Claggett2014ChesapeakeProduction}. We conduct experiments on a single region near Brownsville, Delaware (38°52.66', -75°41.46') that was presented in the author's data exploration example. This region consists of a patchwork of farm fields and forests with diverse appearances. 

In these experiments, we use a simple prediction system to predict the class of unobserved pixels. It is simply a k nearest-neighbor (k=7) classifier that operates on the same PCA-compressed MOSAIKS features that are used for planning. This approach is well-suited to the extremely low number of training samples used in this setting and the standardized and uncorrelated nature of our feature space.

Before any missions have been executed, the agent can only observe the label of the pixel it is at. Then it plans a mission and executes it, observing the labels at the chosen sampling locations. These samples are used to train a prediction model which is used for evaluation and can be used to inform the plan for the next mission. 

The experiments were conducted on 36 random crops drawn from the region in question. Each of the approaches is run on all of the 36 regions individually. Each crop was 6000x6000 pixels, representing a square with 3.6km sides. The agent starts and ends each mission in the center and the path length was set to 3000 pixels (1.8km). This meant that the agent could go halfway to one side of the environment and return to the start within the budget but a significant portion of the domain was unreachable. Each domain was explored using four missions where 10 samples could be collected during each one. Each sample meant the agent could observe the class of a 31x31 pixel (18 meters) square region. After each mission, the class of all pixels is predicted using the KNN classifier, and the error metrics are computed.

The quality of the predictions is evaluated on two metrics: accuracy and averaged recall. The first is the fraction of pixels in the map that were assigned the correct class label by the prediction system. The second represents the average of the per-class recalls. This metric is chosen so that rare classes are treated equally in the evaluation procedure since this is critically important when we explicitly want to find rare classes. We also report the time taken to generate the plan. Note that this does not include the time taken to generate the class predictions, since the planner is agnostic to the choice of prediction algorithm.


\begin{figure}
    \subfloat{\includegraphics[width=0.5\textwidth]{figs/results/ipp/6000/accuracy.pdf}}
    \hfill
    \subfloat{\includegraphics[width=0.5\textwidth]{figs/results/ipp/6000/class-averaged recall.pdf}}
    \hfill
    \caption{Quantitative statistics computed after each flight and averaged across 10 missions. The shaded regions represent the first standard deviation. The left result shows the accuracy and the right shows the class-averaged recall. In both cases higher is better. The proposed methods are only slightly better in terms of error on early flights and the performance converges with the coverage baseline after all flights have been completed. However, the RAPTORS variant that prioritizes rare classes achieves better recall of rare classes after any number of flights. Note that the variance of all methods is high.}
    \label{fig:results:ipp_quant}
\end{figure}

The proposed planner is compared to a hand-designed coverage approach. The path consists of a triangular pattern that begins and ends at the central location. This triangle is sized appropriately to exhaust the available path budget. The goal of this path is to cover a diverse set of locations after the four mapping missions have been completed. This planner is visualized alongside the RAPTORS methods in Figure \ref{fig:results:ipp_pairqual}. As seen there, the RAPTORS methods prioritize collecting a diverse set of samples that span the majority types of land cover. The variant that prioritizes rare classes can be seen to place more samples on the road that cuts across the center of the region.

Quantitative evaluation results are presented Figure~\ref{fig:results:ipp_quant}. For all approaches, the performance improves as more samples are collected. The RAPTORS approaches have slightly better total accuracy than the coverage planner in initial flights, but the performance converges over time. This suggests that the quality of the predictions is saturating, especially for the well-represented classes. This is unsurprising because KNN is a simple prediction model. When these approaches are evaluated on class-averaged recall, which weights rare classes equally, the RAPTORS\_rare method does better for any number of flights. This suggests that it is able to find a diverse set of initial samples and then leverage the predictions after each flight to prioritize sampling regions predicted to be rare classes. However, across all the experiments the scale of the variance is much higher than the difference between approaches. This variance partially captures the difference in difficulty between the different random sites that the approach was evaluated on. This highlights the challenge of evaluating informative path planning in a rigorous manner. The planning time averaged across the RAPTORS variants was 252.2 seconds per path. These experiments were executed on high-end cloud infrastructure, but this shows that the approach has the potential to be deployed in a realistic field scenario in the future.

%\begin{figure}
%    \centering
%    \includegraphics[width=0.6\textwidth]{figs/results/ipp/4000/planning_time.png}
%    \caption{The planning time for each flight. The RAPTORS variants take approximately 200 seconds to plan each flight, while the coverage plan can be computed instantly. }
%    \label{fig:res:ipp_timing}
%\end{figure}